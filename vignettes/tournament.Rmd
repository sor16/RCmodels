---
title: "Tournament - Model comparison"
author: "Sölvi Rögnvaldsson, Axel Örn Jansson, Rafael Vias and Birgir Hrafnkelsson"
date:
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tournament - Model comparison}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width=8, 
  fig.height=6, 
  #fig.path='figs-tournament/',
  fig.align='center',
  prompt=T
)
```

```{r,message=F}
#load the packages needed for this vignette
library(bdrc)
library(ggplot2)
```
This vignette explores the ways you can compare the fit of the different discharge rating curve models provided in the bdrc package. The package includes four different models to fit a discharge rating curve of different complexities. These are:

* plm0() - Power-law model with constant variance (hence the 0). This is a Bayesian hierarchical implementation of the most commonly used discharge rating curve model in hydrological practice

* plm() - Power-law model with variance that may vary with stage

* gplm0() - Generalized power-law model with constant variance (hence the 0)

* gplm() - Generalized power-law model with variance that may vary with stage

To learn more about the four models, see Hrafnkelsson et al. To learn about how to run the models on your data see the [introduction vignette](introduction.html). The tournament function allows you to compare the fit of these four models on the same data and determine which one suits best to the river you are analysing. The tournament is a set of two rounds. The first round consists of two games, one where plm0 and plm are compared and the other where gplm0 and gplm are compared. The winner of each of the two games in round 1 qualify for round 2 where the remaining two models are compared and the winner determined. The comparison of two models is performed by computing the Bayes factor and subsequently the probability of each model given the data. There is a hierarchy in complexity of the models s.t. plm0 < plm < gplm0 < gplm in terms of complexity. For a more complex model to win a game of two models, the probability of model given data must by larger than 0.75. This favors the simpler model except when there is overwhelming evidence of the benefit of the more complex model. As in the introduction vignette, we will use a dataset from a river called x in Sweden that comes with the package:
```{r, data}
data(V316_river)
V316_river
```
The tournament function is easy to use. All you need are two mandatory input arguments, formula and data. The formula is of the form y~x where y is discharge in $m^3/s$ and x is stage in $m$ (it is very important that the data is in the correct units). data is a data.frame which must include x and y as column names. In our case, in V316_river, a column named Q includes are discharge measurements and W the stage measurements. We are ready to run our first tournament:
```{r}
set.seed(1)
t_obj <- tournament(Q~W,V316_river)
```
The function both runs the four models for you and runs the tournament. It's worth mentioning that if you have already run your models of the four different kinds, plm0, plm, gplm0 and gplm and stored them in objects plm0.fit,plm.fit,gplm0.fit and gplm.fit, you can alternatively run the tournament very efficiently in the following way:
```{r,eval=F}
t_obj <- tournament(plm0.fit,plm.fit,gplm0.fit,gplm.fit)
```
The printing method is very simple and gives you the name of the winner
```{r}
t_obj # or alternatively print(t_obj)
```

For a more detailed summary of the results of the tournament write
```{r}
summary(t_obj)
```
Notice here that in round 1, plm was the winner in the first game and gplm in the second. Finally plm was favoured in round 2 and thus determined the tournament winner, the most adequate model for the data at hand.

## Compare different components of the models
To visualize the results from the tournament first we can plot the deviance posterior distribution of the different models. The deviance of an MCMC sample is defined as 2 times the negative log-likelihood of the data given the values of the sampled parameters, thus lower values imply a better fit to the data. From this distribution, DIC and B are calculated. DIC is a metric on the fit of the models taking their complexity into account (number of effective parameters). B is used to calculate the Bayes factor between different models. To plot the deviance distribution we write

```{r}
plot(t_obj,type='deviance') #note that type defaults to deviance
```

rating curves using the plot function
```{r}
plot(t_obj,type='rating_curve')
```
Another useful plot is the residual plot
```{r}
plot(t_obj,type='residuals')
```

The main difference of te four models lies in the modelling of the power law exponent ($f(h)$) the variance on the data level ($\sigma^2_{\varepsilon}$). Thus it is insightful to look at the posterior of the power law exponent for the different models
```{r, fig.width=8, fig.height=6}
plot(t_obj,type='f')
```
and the variance on the data level
```{r}
plot(t_obj,type='sigma_eps')
```

## Customizing tournaments
There are ways to customize the tournament further. If the parameter of zero discharge, $c$, is known and then you want to fix that parameter to the known value in the model. Assume 0.75 m is the known value of $c$ and you want to run a tournament with c fixed for all models. Then you could write
```{r,cache=T}
t_obj_known_c <- tournament(formula=Q~W,data=V316_river,c_param=0.75)
summary(t_obj_known_c )
```




